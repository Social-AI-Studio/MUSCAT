{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39536f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut\n",
    "# from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1142b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c897937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1635it [00:00, 16231.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [01:00, 16394.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1d2328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "my_seed = 79\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)\n",
    "torch.cuda.manual_seed_all(my_seed)\n",
    "\n",
    "df_raw = pd.read_csv(\"pheme_english.csv\", encoding=\"utf-8\")\n",
    "# indo_stopwords = [\"ada\",\"adalah\",\"adanya\",\"adapun\",\"agak\",\"agaknya\",\"agar\",\"akan\",\"akankah\",\"akhir\",\"akhiri\",\"akhirnya\",\"aku\",\"akulah\",\"amat\",\"amatlah\",\"anda\",\"andalah\",\"antar\",\"antara\",\"antaranya\",\"apa\",\"apaan\",\"apabila\",\"apakah\",\"apalagi\",\"apatah\",\"artinya\",\"asal\",\"asalkan\",\"atas\",\"atau\",\"ataukah\",\"ataupun\",\"awal\",\"awalnya\",\"bagai\",\"bagaikan\",\"bagaimana\",\"bagaimanakah\",\"bagaimanapun\",\"bagi\",\"bagian\",\"bahkan\",\"bahwa\",\"bahwasanya\",\"baik\",\"bakal\",\"bakalan\",\"balik\",\"banyak\",\"bapak\",\"baru\",\"bawah\",\"beberapa\",\"begini\",\"beginian\",\"beginikah\",\"beginilah\",\"begitu\",\"begitukah\",\"begitulah\",\"begitupun\",\"bekerja\",\"belakang\",\"belakangan\",\"belum\",\"belumlah\",\"benar\",\"benarkah\",\"benarlah\",\"berada\",\"berakhir\",\"berakhirlah\",\"berakhirnya\",\"berapa\",\"berapakah\",\"berapalah\",\"berapapun\",\"berarti\",\"berawal\",\"berbagai\",\"berdatangan\",\"beri\",\"berikan\",\"berikut\",\"berikutnya\",\"berjumlah\",\"berkali-kali\",\"berkata\",\"berkehendak\",\"berkeinginan\",\"berkenaan\",\"berlainan\",\"berlalu\",\"berlangsung\",\"berlebihan\",\"bermacam\",\"bermacam-macam\",\"bermaksud\",\"bermula\",\"bersama\",\"bersama-sama\",\"bersiap\",\"bersiap-siap\",\"bertanya\",\"bertanya-tanya\",\"berturut\",\"berturut-turut\",\"bertutur\",\"berujar\",\"berupa\",\"besar\",\"betul\",\"betulkah\",\"biasa\",\"biasanya\",\"bila\",\"bilakah\",\"bisa\",\"bisakah\",\"boleh\",\"bolehkah\",\"bolehlah\",\"buat\",\"bukan\",\"bukankah\",\"bukanlah\",\"bukannya\",\"bulan\",\"bung\",\"cara\",\"caranya\",\"cukup\",\"cukupkah\",\"cukuplah\",\"cuma\",\"dahulu\",\"dalam\",\"dan\",\"dapat\",\"dari\",\"daripada\",\"datang\",\"dekat\",\"demi\",\"demikian\",\"demikianlah\",\"dengan\",\"depan\",\"di\",\"dia\",\"diakhiri\",\"diakhirinya\",\"dialah\",\"diantara\",\"diantaranya\",\"diberi\",\"diberikan\",\"diberikannya\",\"dibuat\",\"dibuatnya\",\"didapat\",\"didatangkan\",\"digunakan\",\"diibaratkan\",\"diibaratkannya\",\"diingat\",\"diingatkan\",\"diinginkan\",\"dijawab\",\"dijelaskan\",\"dijelaskannya\",\"dikarenakan\",\"dikatakan\",\"dikatakannya\",\"dikerjakan\",\"diketahui\",\"diketahuinya\",\"dikira\",\"dilakukan\",\"dilalui\",\"dilihat\",\"dimaksud\",\"dimaksudkan\",\"dimaksudkannya\",\"dimaksudnya\",\"diminta\",\"dimintai\",\"dimisalkan\",\"dimulai\",\"dimulailah\",\"dimulainya\",\"dimungkinkan\",\"dini\",\"dipastikan\",\"diperbuat\",\"diperbuatnya\",\"dipergunakan\",\"diperkirakan\",\"diperlihatkan\",\"diperlukan\",\"diperlukannya\",\"dipersoalkan\",\"dipertanyakan\",\"dipunyai\",\"diri\",\"dirinya\",\"disampaikan\",\"disebut\",\"disebutkan\",\"disebutkannya\",\"disini\",\"disinilah\",\"ditambahkan\",\"ditandaskan\",\"ditanya\",\"ditanyai\",\"ditanyakan\",\"ditegaskan\",\"ditujukan\",\"ditunjuk\",\"ditunjuki\",\"ditunjukkan\",\"ditunjukkannya\",\"ditunjuknya\",\"dituturkan\",\"dituturkannya\",\"diucapkan\",\"diucapkannya\",\"diungkapkan\",\"dong\",\"dua\",\"dulu\",\"empat\",\"enggak\",\"enggaknya\",\"entah\",\"entahlah\",\"guna\",\"gunakan\",\"hal\",\"hampir\",\"hanya\",\"hanyalah\",\"hari\",\"harus\",\"haruslah\",\"harusnya\",\"hendak\",\"hendaklah\",\"hendaknya\",\"hingga\",\"ia\",\"ialah\",\"ibarat\",\"ibaratkan\",\"ibaratnya\",\"ibu\",\"ikut\",\"ingat\",\"ingat-ingat\",\"ingin\",\"inginkah\",\"inginkan\",\"ini\",\"inikah\",\"inilah\",\"itu\",\"itukah\",\"itulah\",\"jadi\",\"jadilah\",\"jadinya\",\"jangan\",\"jangankan\",\"janganlah\",\"jauh\",\"jawab\",\"jawaban\",\"jawabnya\",\"jelas\",\"jelaskan\",\"jelaslah\",\"jelasnya\",\"jika\",\"jikalau\",\"juga\",\"jumlah\",\"jumlahnya\",\"justru\",\"kala\",\"kalau\",\"kalaulah\",\"kalaupun\",\"kalian\",\"kami\",\"kamilah\",\"kamu\",\"kamulah\",\"kan\",\"kapan\",\"kapankah\",\"kapanpun\",\"karena\",\"karenanya\",\"kasus\",\"kata\",\"katakan\",\"katakanlah\",\"katanya\",\"ke\",\"keadaan\",\"kebetulan\",\"kecil\",\"kedua\",\"keduanya\",\"keinginan\",\"kelamaan\",\"kelihatan\",\"kelihatannya\",\"kelima\",\"keluar\",\"kembali\",\"kemudian\",\"kemungkinan\",\"kemungkinannya\",\"kenapa\",\"kepada\",\"kepadanya\",\"kesampaian\",\"keseluruhan\",\"keseluruhannya\",\"keterlaluan\",\"ketika\",\"khususnya\",\"kini\",\"kinilah\",\"kira\",\"kira-kira\",\"kiranya\",\"kita\",\"kitalah\",\"kok\",\"kurang\",\"lagi\",\"lagian\",\"lah\",\"lain\",\"lainnya\",\"lalu\",\"lama\",\"lamanya\",\"lanjut\",\"lanjutnya\",\"lebih\",\"lewat\",\"lima\",\"luar\",\"macam\",\"maka\",\"makanya\",\"makin\",\"malah\",\"malahan\",\"mampu\",\"mampukah\",\"mana\",\"manakala\",\"manalagi\",\"masa\",\"masalah\",\"masalahnya\",\"masih\",\"masihkah\",\"masing\",\"masing-masing\",\"mau\",\"maupun\",\"melainkan\",\"melakukan\",\"melalui\",\"melihat\",\"melihatnya\",\"memang\",\"memastikan\",\"memberi\",\"memberikan\",\"membuat\",\"memerlukan\",\"memihak\",\"meminta\",\"memintakan\",\"memisalkan\",\"memperbuat\",\"mempergunakan\",\"memperkirakan\",\"memperlihatkan\",\"mempersiapkan\",\"mempersoalkan\",\"mempertanyakan\",\"mempunyai\",\"memulai\",\"memungkinkan\",\"menaiki\",\"menambahkan\",\"menandaskan\",\"menanti\",\"menanti-nanti\",\"menantikan\",\"menanya\",\"menanyai\",\"menanyakan\",\"mendapat\",\"mendapatkan\",\"mendatang\",\"mendatangi\",\"mendatangkan\",\"menegaskan\",\"mengakhiri\",\"mengapa\",\"mengatakan\",\"mengatakannya\",\"mengenai\",\"mengerjakan\",\"mengetahui\",\"menggunakan\",\"menghendaki\",\"mengibaratkan\",\"mengibaratkannya\",\"mengingat\",\"mengingatkan\",\"menginginkan\",\"mengira\",\"mengucapkan\",\"mengucapkannya\",\"mengungkapkan\",\"menjadi\",\"menjawab\",\"menjelaskan\",\"menuju\",\"menunjuk\",\"menunjuki\",\"menunjukkan\",\"menunjuknya\",\"menurut\",\"menuturkan\",\"menyampaikan\",\"menyangkut\",\"menyatakan\",\"menyebutkan\",\"menyeluruh\",\"menyiapkan\",\"merasa\",\"mereka\",\"merekalah\",\"merupakan\",\"meski\",\"meskipun\",\"meyakini\",\"meyakinkan\",\"minta\",\"mirip\",\"misal\",\"misalkan\",\"misalnya\",\"mula\",\"mulai\",\"mulailah\",\"mulanya\",\"mungkin\",\"mungkinkah\",\"nah\",\"naik\",\"namun\",\"nanti\",\"nantinya\",\"nyaris\",\"nyatanya\",\"oleh\",\"olehnya\",\"pada\",\"padahal\",\"padanya\",\"pak\",\"paling\",\"panjang\",\"pantas\",\"para\",\"pasti\",\"pastilah\",\"penting\",\"pentingnya\",\"per\",\"percuma\",\"perlu\",\"perlukah\",\"perlunya\",\"pernah\",\"persoalan\",\"pertama\",\"pertama-tama\",\"pertanyaan\",\"pertanyakan\",\"pihak\",\"pihaknya\",\"pukul\",\"pula\",\"pun\",\"punya\",\"rasa\",\"rasanya\",\"rata\",\"rupanya\",\"saat\",\"saatnya\",\"saja\",\"sajalah\",\"saling\",\"sama\",\"sama-sama\",\"sambil\",\"sampai\",\"sampai-sampai\",\"sampaikan\",\"sana\",\"sangat\",\"sangatlah\",\"satu\",\"saya\",\"sayalah\",\"se\",\"sebab\",\"sebabnya\",\"sebagai\",\"sebagaimana\",\"sebagainya\",\"sebagian\",\"sebaik\",\"sebaik-baiknya\",\"sebaiknya\",\"sebaliknya\",\"sebanyak\",\"sebegini\",\"sebegitu\",\"sebelum\",\"sebelumnya\",\"sebenarnya\",\"seberapa\",\"sebesar\",\"sebetulnya\",\"sebisanya\",\"sebuah\",\"sebut\",\"sebutlah\",\"sebutnya\",\"secara\",\"secukupnya\",\"sedang\",\"sedangkan\",\"sedemikian\",\"sedikit\",\"sedikitnya\",\"seenaknya\",\"segala\",\"segalanya\",\"segera\",\"seharusnya\",\"sehingga\",\"seingat\",\"sejak\",\"sejauh\",\"sejenak\",\"sejumlah\",\"sekadar\",\"sekadarnya\",\"sekali\",\"sekali-kali\",\"sekalian\",\"sekaligus\",\"sekalipun\",\"sekarang\",\"sekecil\",\"seketika\",\"sekiranya\",\"sekitar\",\"sekitarnya\",\"sekurang-kurangnya\",\"sekurangnya\",\"sela\",\"selagi\",\"selain\",\"selaku\",\"selalu\",\"selama\",\"selama-lamanya\",\"selamanya\",\"selanjutnya\",\"seluruh\",\"seluruhnya\",\"semacam\",\"semakin\",\"semampu\",\"semampunya\",\"semasa\",\"semasih\",\"semata\",\"semata-mata\",\"semaunya\",\"sementara\",\"semisal\",\"semisalnya\",\"sempat\",\"semua\",\"semuanya\",\"semula\",\"sendiri\",\"sendirian\",\"sendirinya\",\"seolah\",\"seolah-olah\",\"seorang\",\"sepanjang\",\"sepantasnya\",\"sepantasnyalah\",\"seperlunya\",\"seperti\",\"sepertinya\",\"sepihak\",\"sering\",\"seringnya\",\"serta\",\"serupa\",\"sesaat\",\"sesama\",\"sesampai\",\"sesegera\",\"sesekali\",\"seseorang\",\"sesuatu\",\"sesuatunya\",\"sesudah\",\"sesudahnya\",\"setelah\",\"setempat\",\"setengah\",\"seterusnya\",\"setiap\",\"setiba\",\"setibanya\",\"setidak-tidaknya\",\"setidaknya\",\"setinggi\",\"seusai\",\"sewaktu\",\"siap\",\"siapa\",\"siapakah\",\"siapapun\",\"sini\",\"sinilah\",\"soal\",\"soalnya\",\"suatu\",\"sudah\",\"sudahkah\",\"sudahlah\",\"supaya\",\"tadi\",\"tadinya\",\"tahu\",\"tahun\",\"tak\",\"tambah\",\"tambahnya\",\"tampak\",\"tampaknya\",\"tandas\",\"tandasnya\",\"tanpa\",\"tanya\",\"tanyakan\",\"tanyanya\",\"tapi\",\"tegas\",\"tegasnya\",\"telah\",\"tempat\",\"tengah\",\"tentang\",\"tentu\",\"tentulah\",\"tentunya\",\"tepat\",\"terakhir\",\"terasa\",\"terbanyak\",\"terdahulu\",\"terdapat\",\"terdiri\",\"terhadap\",\"terhadapnya\",\"teringat\",\"teringat-ingat\",\"terjadi\",\"terjadilah\",\"terjadinya\",\"terkira\",\"terlalu\",\"terlebih\",\"terlihat\",\"termasuk\",\"ternyata\",\"tersampaikan\",\"tersebut\",\"tersebutlah\",\"tertentu\",\"tertuju\",\"terus\",\"terutama\",\"tetap\",\"tetapi\",\"tiap\",\"tiba\",\"tiba-tiba\",\"tidak\",\"tidakkah\",\"tidaklah\",\"tiga\",\"tinggi\",\"toh\",\"tunjuk\",\"turut\",\"tutur\",\"tuturnya\",\"ucap\",\"ucapnya\",\"ujar\",\"ujarnya\",\"umum\",\"umumnya\",\"ungkap\",\"ungkapnya\",\"untuk\",\"usah\",\"usai\",\"waduh\",\"wah\",\"wahai\",\"waktu\",\"waktunya\",\"walau\",\"walaupun\",\"wong\",\"yaitu\",\"yakin\",\"yakni\",\"yang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae8c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"label\": df_raw['label'], \"content\": df_raw[\"full_content\"], \"event\": df_raw['event']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f41304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509cc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a02d86c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = df['full_content']\n",
    "# y = df['label']\n",
    "# groups = df['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b171c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "# print(f'shape of train data is {x_train.shape}')\n",
    "# print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a29e1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# def tokenize_data(example, tokenizer, max_length):\n",
    "#     tokens = tokenizer(example['text'])[:max_length]\n",
    "#     length = len(tokens)\n",
    "#     return {'tokens': tokens, 'length': length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f987341",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = [word for word in tknzr.tokenize(text)]\n",
    "    # stems = [stemmer.stem(item) for item in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84742730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 458  459  460 ... 6422 6423 6424] TEST: [   0    1    2 ... 4020 4021 4022]\n"
     ]
    }
   ],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "X = df['content']\n",
    "y = df['label']\n",
    "groups = df['event']\n",
    "\n",
    "train_index, test_index = next(logo.split(X, y, groups))\n",
    "print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index].values, y[test_index].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ed624f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458     Breaking news: Ghana international and AC Mila...\n",
      "459     Unconfirmed reports claim that Michael Essien ...\n",
      "460     BREAKING: Unconfirmed reports claim AC Milan m...\n",
      "461     Milan have stated that the reports about Essie...\n",
      "462     AC Milan midfielder Michael Essien has been di...\n",
      "                              ...                        \n",
      "6420    Sydney siege ends as police storm Lindt Cafe a...\n",
      "6421    Breaking News: #SydneySiege is over, according...\n",
      "6422    Watch gunfire erupt and hostages flee chocolat...\n",
      "6423    Authorities have confirmed that #sydneysiege i...\n",
      "6424    WATCH: The dramatic moment tactical teams stor...\n",
      "Name: content, Length: 4346, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70dc2dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# df['doc_len'] = df['content'].apply(lambda words: len(words.split(\" \")))\n",
    "# max_seq_len = 1000\n",
    "# print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037945dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5238402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4346/4346 [00:02<00:00, 1770.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2079/2079 [00:01<00:00, 1498.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  90971\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 1000\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "raw_docs_train = X_train.tolist()\n",
    "raw_docs_test = X_test.tolist()\n",
    "num_classes = 4\n",
    "\n",
    "processed_docs_train = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_train.append(\" \".join(filtered))\n",
    "    \n",
    "processed_docs_test = []\n",
    "\n",
    "for doc in tqdm(raw_docs_test):\n",
    "    tokens = tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_test.append(\" \".join(filtered))\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c4f4f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes = 4)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes = 4)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e570b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458     Breaking news: Ghana international and AC Mila...\n",
      "459     Unconfirmed reports claim that Michael Essien ...\n",
      "460     BREAKING: Unconfirmed reports claim AC Milan m...\n",
      "461     Milan have stated that the reports about Essie...\n",
      "462     AC Milan midfielder Michael Essien has been di...\n",
      "                              ...                        \n",
      "6420    Sydney siege ends as police storm Lindt Cafe a...\n",
      "6421    Breaking News: #SydneySiege is over, according...\n",
      "6422    Watch gunfire erupt and hostages flee chocolat...\n",
      "6423    Authorities have confirmed that #sydneysiege i...\n",
      "6424    WATCH: The dramatic moment tactical teams stor...\n",
      "Name: content, Length: 4346, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfd45516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 32\n",
    "num_epochs = 40\n",
    "\n",
    "#model parameters\n",
    "num_filters = 16 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa68f693",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "50000\n",
      "26633\n"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix...')\n",
    "\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "print(nb_words)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     print(word)\n",
    "#     print(embedding_vector)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        #print(\"yes\")\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print(len(words_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3da28e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1000, 300)         15000000  \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               85248     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,087,460\n",
      "Trainable params: 15,087,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=True))\n",
    "# model.add(Embedding(10000,300,input_length=1000,trainable=True))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "581a7ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 1000), (None, 4)), types: (tf.int32, tf.float32)>\n",
      "Epoch 1/40\n",
      "68/68 [==============================] - 13s 116ms/step - loss: 1.1109 - accuracy: 0.5446 - val_loss: 0.9711 - val_accuracy: 0.7797\n",
      "Epoch 2/40\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 1.4937 - accuracy: 0.5219 - val_loss: 0.8139 - val_accuracy: 0.7797\n",
      "Epoch 3/40\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 1.2249 - accuracy: 0.5306 - val_loss: 0.7649 - val_accuracy: 0.7797\n",
      "Epoch 4/40\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 1.0039 - accuracy: 0.6420 - val_loss: 0.7669 - val_accuracy: 0.7802\n",
      "Epoch 5/40\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.7265 - accuracy: 0.7678 - val_loss: 0.8320 - val_accuracy: 0.7744\n",
      "Epoch 6/40\n",
      "68/68 [==============================] - 8s 114ms/step - loss: 0.5358 - accuracy: 0.8318 - val_loss: 0.9194 - val_accuracy: 0.7638\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((word_seq_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((word_seq_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(train_dataset,\n",
    "          epochs=num_epochs, validation_data=test_dataset, callbacks=[es_callback], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4258a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
