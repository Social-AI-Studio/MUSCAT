{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1829280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut\n",
    "# from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import codecs\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c59f73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d1a4687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1611it [00:00, 15993.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:04, 16128.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2000000 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('cc.id.300.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5567acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "my_seed = 79\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)\n",
    "torch.cuda.manual_seed_all(my_seed)\n",
    "\n",
    "df_raw = pd.read_csv(\"pheme_indo.csv\", encoding=\"utf-8\")\n",
    "# indo_stopwords = [\"ada\",\"adalah\",\"adanya\",\"adapun\",\"agak\",\"agaknya\",\"agar\",\"akan\",\"akankah\",\"akhir\",\"akhiri\",\"akhirnya\",\"aku\",\"akulah\",\"amat\",\"amatlah\",\"anda\",\"andalah\",\"antar\",\"antara\",\"antaranya\",\"apa\",\"apaan\",\"apabila\",\"apakah\",\"apalagi\",\"apatah\",\"artinya\",\"asal\",\"asalkan\",\"atas\",\"atau\",\"ataukah\",\"ataupun\",\"awal\",\"awalnya\",\"bagai\",\"bagaikan\",\"bagaimana\",\"bagaimanakah\",\"bagaimanapun\",\"bagi\",\"bagian\",\"bahkan\",\"bahwa\",\"bahwasanya\",\"baik\",\"bakal\",\"bakalan\",\"balik\",\"banyak\",\"bapak\",\"baru\",\"bawah\",\"beberapa\",\"begini\",\"beginian\",\"beginikah\",\"beginilah\",\"begitu\",\"begitukah\",\"begitulah\",\"begitupun\",\"bekerja\",\"belakang\",\"belakangan\",\"belum\",\"belumlah\",\"benar\",\"benarkah\",\"benarlah\",\"berada\",\"berakhir\",\"berakhirlah\",\"berakhirnya\",\"berapa\",\"berapakah\",\"berapalah\",\"berapapun\",\"berarti\",\"berawal\",\"berbagai\",\"berdatangan\",\"beri\",\"berikan\",\"berikut\",\"berikutnya\",\"berjumlah\",\"berkali-kali\",\"berkata\",\"berkehendak\",\"berkeinginan\",\"berkenaan\",\"berlainan\",\"berlalu\",\"berlangsung\",\"berlebihan\",\"bermacam\",\"bermacam-macam\",\"bermaksud\",\"bermula\",\"bersama\",\"bersama-sama\",\"bersiap\",\"bersiap-siap\",\"bertanya\",\"bertanya-tanya\",\"berturut\",\"berturut-turut\",\"bertutur\",\"berujar\",\"berupa\",\"besar\",\"betul\",\"betulkah\",\"biasa\",\"biasanya\",\"bila\",\"bilakah\",\"bisa\",\"bisakah\",\"boleh\",\"bolehkah\",\"bolehlah\",\"buat\",\"bukan\",\"bukankah\",\"bukanlah\",\"bukannya\",\"bulan\",\"bung\",\"cara\",\"caranya\",\"cukup\",\"cukupkah\",\"cukuplah\",\"cuma\",\"dahulu\",\"dalam\",\"dan\",\"dapat\",\"dari\",\"daripada\",\"datang\",\"dekat\",\"demi\",\"demikian\",\"demikianlah\",\"dengan\",\"depan\",\"di\",\"dia\",\"diakhiri\",\"diakhirinya\",\"dialah\",\"diantara\",\"diantaranya\",\"diberi\",\"diberikan\",\"diberikannya\",\"dibuat\",\"dibuatnya\",\"didapat\",\"didatangkan\",\"digunakan\",\"diibaratkan\",\"diibaratkannya\",\"diingat\",\"diingatkan\",\"diinginkan\",\"dijawab\",\"dijelaskan\",\"dijelaskannya\",\"dikarenakan\",\"dikatakan\",\"dikatakannya\",\"dikerjakan\",\"diketahui\",\"diketahuinya\",\"dikira\",\"dilakukan\",\"dilalui\",\"dilihat\",\"dimaksud\",\"dimaksudkan\",\"dimaksudkannya\",\"dimaksudnya\",\"diminta\",\"dimintai\",\"dimisalkan\",\"dimulai\",\"dimulailah\",\"dimulainya\",\"dimungkinkan\",\"dini\",\"dipastikan\",\"diperbuat\",\"diperbuatnya\",\"dipergunakan\",\"diperkirakan\",\"diperlihatkan\",\"diperlukan\",\"diperlukannya\",\"dipersoalkan\",\"dipertanyakan\",\"dipunyai\",\"diri\",\"dirinya\",\"disampaikan\",\"disebut\",\"disebutkan\",\"disebutkannya\",\"disini\",\"disinilah\",\"ditambahkan\",\"ditandaskan\",\"ditanya\",\"ditanyai\",\"ditanyakan\",\"ditegaskan\",\"ditujukan\",\"ditunjuk\",\"ditunjuki\",\"ditunjukkan\",\"ditunjukkannya\",\"ditunjuknya\",\"dituturkan\",\"dituturkannya\",\"diucapkan\",\"diucapkannya\",\"diungkapkan\",\"dong\",\"dua\",\"dulu\",\"empat\",\"enggak\",\"enggaknya\",\"entah\",\"entahlah\",\"guna\",\"gunakan\",\"hal\",\"hampir\",\"hanya\",\"hanyalah\",\"hari\",\"harus\",\"haruslah\",\"harusnya\",\"hendak\",\"hendaklah\",\"hendaknya\",\"hingga\",\"ia\",\"ialah\",\"ibarat\",\"ibaratkan\",\"ibaratnya\",\"ibu\",\"ikut\",\"ingat\",\"ingat-ingat\",\"ingin\",\"inginkah\",\"inginkan\",\"ini\",\"inikah\",\"inilah\",\"itu\",\"itukah\",\"itulah\",\"jadi\",\"jadilah\",\"jadinya\",\"jangan\",\"jangankan\",\"janganlah\",\"jauh\",\"jawab\",\"jawaban\",\"jawabnya\",\"jelas\",\"jelaskan\",\"jelaslah\",\"jelasnya\",\"jika\",\"jikalau\",\"juga\",\"jumlah\",\"jumlahnya\",\"justru\",\"kala\",\"kalau\",\"kalaulah\",\"kalaupun\",\"kalian\",\"kami\",\"kamilah\",\"kamu\",\"kamulah\",\"kan\",\"kapan\",\"kapankah\",\"kapanpun\",\"karena\",\"karenanya\",\"kasus\",\"kata\",\"katakan\",\"katakanlah\",\"katanya\",\"ke\",\"keadaan\",\"kebetulan\",\"kecil\",\"kedua\",\"keduanya\",\"keinginan\",\"kelamaan\",\"kelihatan\",\"kelihatannya\",\"kelima\",\"keluar\",\"kembali\",\"kemudian\",\"kemungkinan\",\"kemungkinannya\",\"kenapa\",\"kepada\",\"kepadanya\",\"kesampaian\",\"keseluruhan\",\"keseluruhannya\",\"keterlaluan\",\"ketika\",\"khususnya\",\"kini\",\"kinilah\",\"kira\",\"kira-kira\",\"kiranya\",\"kita\",\"kitalah\",\"kok\",\"kurang\",\"lagi\",\"lagian\",\"lah\",\"lain\",\"lainnya\",\"lalu\",\"lama\",\"lamanya\",\"lanjut\",\"lanjutnya\",\"lebih\",\"lewat\",\"lima\",\"luar\",\"macam\",\"maka\",\"makanya\",\"makin\",\"malah\",\"malahan\",\"mampu\",\"mampukah\",\"mana\",\"manakala\",\"manalagi\",\"masa\",\"masalah\",\"masalahnya\",\"masih\",\"masihkah\",\"masing\",\"masing-masing\",\"mau\",\"maupun\",\"melainkan\",\"melakukan\",\"melalui\",\"melihat\",\"melihatnya\",\"memang\",\"memastikan\",\"memberi\",\"memberikan\",\"membuat\",\"memerlukan\",\"memihak\",\"meminta\",\"memintakan\",\"memisalkan\",\"memperbuat\",\"mempergunakan\",\"memperkirakan\",\"memperlihatkan\",\"mempersiapkan\",\"mempersoalkan\",\"mempertanyakan\",\"mempunyai\",\"memulai\",\"memungkinkan\",\"menaiki\",\"menambahkan\",\"menandaskan\",\"menanti\",\"menanti-nanti\",\"menantikan\",\"menanya\",\"menanyai\",\"menanyakan\",\"mendapat\",\"mendapatkan\",\"mendatang\",\"mendatangi\",\"mendatangkan\",\"menegaskan\",\"mengakhiri\",\"mengapa\",\"mengatakan\",\"mengatakannya\",\"mengenai\",\"mengerjakan\",\"mengetahui\",\"menggunakan\",\"menghendaki\",\"mengibaratkan\",\"mengibaratkannya\",\"mengingat\",\"mengingatkan\",\"menginginkan\",\"mengira\",\"mengucapkan\",\"mengucapkannya\",\"mengungkapkan\",\"menjadi\",\"menjawab\",\"menjelaskan\",\"menuju\",\"menunjuk\",\"menunjuki\",\"menunjukkan\",\"menunjuknya\",\"menurut\",\"menuturkan\",\"menyampaikan\",\"menyangkut\",\"menyatakan\",\"menyebutkan\",\"menyeluruh\",\"menyiapkan\",\"merasa\",\"mereka\",\"merekalah\",\"merupakan\",\"meski\",\"meskipun\",\"meyakini\",\"meyakinkan\",\"minta\",\"mirip\",\"misal\",\"misalkan\",\"misalnya\",\"mula\",\"mulai\",\"mulailah\",\"mulanya\",\"mungkin\",\"mungkinkah\",\"nah\",\"naik\",\"namun\",\"nanti\",\"nantinya\",\"nyaris\",\"nyatanya\",\"oleh\",\"olehnya\",\"pada\",\"padahal\",\"padanya\",\"pak\",\"paling\",\"panjang\",\"pantas\",\"para\",\"pasti\",\"pastilah\",\"penting\",\"pentingnya\",\"per\",\"percuma\",\"perlu\",\"perlukah\",\"perlunya\",\"pernah\",\"persoalan\",\"pertama\",\"pertama-tama\",\"pertanyaan\",\"pertanyakan\",\"pihak\",\"pihaknya\",\"pukul\",\"pula\",\"pun\",\"punya\",\"rasa\",\"rasanya\",\"rata\",\"rupanya\",\"saat\",\"saatnya\",\"saja\",\"sajalah\",\"saling\",\"sama\",\"sama-sama\",\"sambil\",\"sampai\",\"sampai-sampai\",\"sampaikan\",\"sana\",\"sangat\",\"sangatlah\",\"satu\",\"saya\",\"sayalah\",\"se\",\"sebab\",\"sebabnya\",\"sebagai\",\"sebagaimana\",\"sebagainya\",\"sebagian\",\"sebaik\",\"sebaik-baiknya\",\"sebaiknya\",\"sebaliknya\",\"sebanyak\",\"sebegini\",\"sebegitu\",\"sebelum\",\"sebelumnya\",\"sebenarnya\",\"seberapa\",\"sebesar\",\"sebetulnya\",\"sebisanya\",\"sebuah\",\"sebut\",\"sebutlah\",\"sebutnya\",\"secara\",\"secukupnya\",\"sedang\",\"sedangkan\",\"sedemikian\",\"sedikit\",\"sedikitnya\",\"seenaknya\",\"segala\",\"segalanya\",\"segera\",\"seharusnya\",\"sehingga\",\"seingat\",\"sejak\",\"sejauh\",\"sejenak\",\"sejumlah\",\"sekadar\",\"sekadarnya\",\"sekali\",\"sekali-kali\",\"sekalian\",\"sekaligus\",\"sekalipun\",\"sekarang\",\"sekecil\",\"seketika\",\"sekiranya\",\"sekitar\",\"sekitarnya\",\"sekurang-kurangnya\",\"sekurangnya\",\"sela\",\"selagi\",\"selain\",\"selaku\",\"selalu\",\"selama\",\"selama-lamanya\",\"selamanya\",\"selanjutnya\",\"seluruh\",\"seluruhnya\",\"semacam\",\"semakin\",\"semampu\",\"semampunya\",\"semasa\",\"semasih\",\"semata\",\"semata-mata\",\"semaunya\",\"sementara\",\"semisal\",\"semisalnya\",\"sempat\",\"semua\",\"semuanya\",\"semula\",\"sendiri\",\"sendirian\",\"sendirinya\",\"seolah\",\"seolah-olah\",\"seorang\",\"sepanjang\",\"sepantasnya\",\"sepantasnyalah\",\"seperlunya\",\"seperti\",\"sepertinya\",\"sepihak\",\"sering\",\"seringnya\",\"serta\",\"serupa\",\"sesaat\",\"sesama\",\"sesampai\",\"sesegera\",\"sesekali\",\"seseorang\",\"sesuatu\",\"sesuatunya\",\"sesudah\",\"sesudahnya\",\"setelah\",\"setempat\",\"setengah\",\"seterusnya\",\"setiap\",\"setiba\",\"setibanya\",\"setidak-tidaknya\",\"setidaknya\",\"setinggi\",\"seusai\",\"sewaktu\",\"siap\",\"siapa\",\"siapakah\",\"siapapun\",\"sini\",\"sinilah\",\"soal\",\"soalnya\",\"suatu\",\"sudah\",\"sudahkah\",\"sudahlah\",\"supaya\",\"tadi\",\"tadinya\",\"tahu\",\"tahun\",\"tak\",\"tambah\",\"tambahnya\",\"tampak\",\"tampaknya\",\"tandas\",\"tandasnya\",\"tanpa\",\"tanya\",\"tanyakan\",\"tanyanya\",\"tapi\",\"tegas\",\"tegasnya\",\"telah\",\"tempat\",\"tengah\",\"tentang\",\"tentu\",\"tentulah\",\"tentunya\",\"tepat\",\"terakhir\",\"terasa\",\"terbanyak\",\"terdahulu\",\"terdapat\",\"terdiri\",\"terhadap\",\"terhadapnya\",\"teringat\",\"teringat-ingat\",\"terjadi\",\"terjadilah\",\"terjadinya\",\"terkira\",\"terlalu\",\"terlebih\",\"terlihat\",\"termasuk\",\"ternyata\",\"tersampaikan\",\"tersebut\",\"tersebutlah\",\"tertentu\",\"tertuju\",\"terus\",\"terutama\",\"tetap\",\"tetapi\",\"tiap\",\"tiba\",\"tiba-tiba\",\"tidak\",\"tidakkah\",\"tidaklah\",\"tiga\",\"tinggi\",\"toh\",\"tunjuk\",\"turut\",\"tutur\",\"tuturnya\",\"ucap\",\"ucapnya\",\"ujar\",\"ujarnya\",\"umum\",\"umumnya\",\"ungkap\",\"ungkapnya\",\"untuk\",\"usah\",\"usai\",\"waduh\",\"wah\",\"wahai\",\"waktu\",\"waktunya\",\"walau\",\"walaupun\",\"wong\",\"yaitu\",\"yakin\",\"yakni\",\"yang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3e884c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"label\": df_raw['label'], \"content\": df_raw[\"full_content\"], \"event\": df_raw['event']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9d8a325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753b1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da34b7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = df['full_content']\n",
    "# y = df['label']\n",
    "# groups = df['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80631ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "# print(f'shape of train data is {x_train.shape}')\n",
    "# print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb96f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# def tokenize_data(example, tokenizer, max_length):\n",
    "#     tokens = tokenizer(example['text'])[:max_length]\n",
    "#     length = len(tokens)\n",
    "#     return {'tokens': tokens, 'length': length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc53bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "\n",
    "def tokenize(text):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = [word for word in tknzr.tokenize(text)]\n",
    "    # stems = [stemmer.stem(item) for item in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2640b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "X = df['content']\n",
    "y = df['label']\n",
    "groups = df['event']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "497a5606",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3746/3746 [00:01<00:00, 2301.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2002/2002 [00:00<00:00, 2060.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  63388\n",
      "preparing embedding matrix...\n",
      "50000\n",
      "27255\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_22 (Embedding)    (None, 1000, 300)         15000000  \n",
      "                                                                 \n",
      " spatial_dropout1d_22 (Spati  (None, 1000, 300)        0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " bidirectional_22 (Bidirecti  (None, 64)               85248     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,086,356\n",
      "Trainable params: 15,086,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<BatchDataset shapes: ((None, 1000), (None, 4)), types: (tf.int32, tf.float32)>\n",
      "Epoch 1/40\n",
      "59/59 [==============================] - 10s 132ms/step - loss: 1.2010 - accuracy: 0.5443 - val_loss: 0.8315 - val_accuracy: 0.7767\n",
      "Epoch 2/40\n",
      "59/59 [==============================] - 7s 125ms/step - loss: 1.0366 - accuracy: 0.5862 - val_loss: 0.8341 - val_accuracy: 0.7343\n",
      "Epoch 3/40\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 0.7346 - accuracy: 0.7018 - val_loss: 1.0588 - val_accuracy: 0.5884\n",
      "Epoch 4/40\n",
      "59/59 [==============================] - 7s 127ms/step - loss: 0.5414 - accuracy: 0.7656 - val_loss: 1.4119 - val_accuracy: 0.5030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edge\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  4%|██▉                                                                          | 221/5734 [00:00<00:02, 2197.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 1 3 3]\n",
      "[1 1 1 ... 3 3 3]\n",
      "[1 1 1 ... 3 3 3]\n",
      "(0.2396898051710587, 0.29916156691906093, 0.22193723679592908, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5734/5734 [00:02<00:00, 2220.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 2793.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  63388\n",
      "preparing embedding matrix...\n",
      "50000\n",
      "27282\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, 1000, 300)         15000000  \n",
      "                                                                 \n",
      " spatial_dropout1d_23 (Spati  (None, 1000, 300)        0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " bidirectional_23 (Bidirecti  (None, 64)               85248     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,086,356\n",
      "Trainable params: 15,086,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<BatchDataset shapes: ((None, 1000), (None, 4)), types: (tf.int32, tf.float32)>\n",
      "Epoch 1/40\n",
      "90/90 [==============================] - 13s 124ms/step - loss: 1.0862 - accuracy: 0.6230 - val_loss: 1.7493 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "90/90 [==============================] - 10s 111ms/step - loss: 0.8330 - accuracy: 0.6800 - val_loss: 2.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 0.5904 - accuracy: 0.7625 - val_loss: 2.8676 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/40\n",
      "90/90 [==============================] - 11s 117ms/step - loss: 0.4284 - accuracy: 0.8348 - val_loss: 2.1650 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edge\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\edge\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  5%|███▌                                                                         | 222/4738 [00:00<00:02, 2213.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 1 2 2 1 2 2 3 3 3 3 2 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4738/4738 [00:01<00:00, 2501.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:00<00:00, 1620.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  63388\n",
      "preparing embedding matrix...\n",
      "50000\n",
      "27338\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_24 (Embedding)    (None, 1000, 300)         15000000  \n",
      "                                                                 \n",
      " spatial_dropout1d_24 (Spati  (None, 1000, 300)        0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " bidirectional_24 (Bidirecti  (None, 64)               85248     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,086,356\n",
      "Trainable params: 15,086,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<BatchDataset shapes: ((None, 1000), (None, 4)), types: (tf.int32, tf.float32)>\n",
      "Epoch 1/40\n",
      "75/75 [==============================] - 11s 122ms/step - loss: 1.1050 - accuracy: 0.5990 - val_loss: 1.0214 - val_accuracy: 0.7455\n",
      "Epoch 2/40\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.8738 - accuracy: 0.6657 - val_loss: 1.0430 - val_accuracy: 0.6634\n",
      "Epoch 3/40\n",
      "75/75 [==============================] - 9s 115ms/step - loss: 0.6038 - accuracy: 0.7615 - val_loss: 1.2358 - val_accuracy: 0.5792\n",
      "Epoch 4/40\n",
      "75/75 [==============================] - 9s 119ms/step - loss: 0.4695 - accuracy: 0.8172 - val_loss: 1.5178 - val_accuracy: 0.6772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edge\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  0%|                                                                                         | 0/5343 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 3 ... 3 3 3]\n",
      "[0 2 2 ... 3 3 3]\n",
      "[0 2 2 ... 3 3 3]\n",
      "(0.1927846674182638, 0.22709163346613545, 0.20853658536585365, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5343/5343 [00:02<00:00, 2145.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 405/405 [00:00<00:00, 3904.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  63388\n",
      "preparing embedding matrix...\n",
      "50000\n",
      "27278\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, 1000, 300)         15000000  \n",
      "                                                                 \n",
      " spatial_dropout1d_25 (Spati  (None, 1000, 300)        0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " bidirectional_25 (Bidirecti  (None, 64)               85248     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,086,356\n",
      "Trainable params: 15,086,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<BatchDataset shapes: ((None, 1000), (None, 4)), types: (tf.int32, tf.float32)>\n",
      "Epoch 1/40\n",
      "84/84 [==============================] - 13s 130ms/step - loss: 1.0779 - accuracy: 0.6380 - val_loss: 1.2387 - val_accuracy: 0.5012\n",
      "Epoch 2/40\n",
      "84/84 [==============================] - 10s 124ms/step - loss: 0.8336 - accuracy: 0.6899 - val_loss: 1.3231 - val_accuracy: 0.4840\n",
      "Epoch 3/40\n",
      "84/84 [==============================] - 10s 114ms/step - loss: 0.5757 - accuracy: 0.7707 - val_loss: 1.6892 - val_accuracy: 0.4667\n",
      "Epoch 4/40\n",
      "84/84 [==============================] - 10s 117ms/step - loss: 0.4145 - accuracy: 0.8417 - val_loss: 2.3977 - val_accuracy: 0.4790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edge\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  4%|███▏                                                                         | 237/5733 [00:00<00:02, 2353.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 2 2 3 2 3 2 1 1 2 3 1 3 3 1 3 3 3 2 3 1 3 3 3 2 3 3 2 1 3 3 3 3 3\n",
      " 3 1 3 2 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 1 3 3 3 3 1 1 2 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 3 2 1 3 3 2 1 3 1 3\n",
      " 2 2 3 3 3 3 1 3 3 1 3 1 3 2 1 3 3 3 1 3 3 1 3 3 3 3 1 3 3 2 3 2 3 2 2 1 1\n",
      " 3 3 3 3 3 3 1 2 2 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 2 3 3 1 3 3 3\n",
      " 3 3 2 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 2 1 1 3 2 3 3 3\n",
      " 3 3 3 3 3 3 3 1 3 3 2 3 3 3 3 3 3 3 3 3 1 2 3 3 3 3 3 3 3 3 2 3 3 1 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 1 1 2 3 3 3 3 3 2 2 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 2 3\n",
      " 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3\n",
      " 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3\n",
      " 3 3 3 1 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[2 0 1 1 1 2 0 1 1 2 0 1 1 1 1 1 2 0 0 2 2 1 2 2 2 1 2 1 1 1 2 1 1 0 2 0 1\n",
      " 2 0 1 2 1 1 0 1 1 0 2 0 1 1 0 0 2 2 1 1 1 1 1 1 1 0 1 0 2 1 0 1 2 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 2 0 0 0 1 0 1 0\n",
      " 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 2 2 2 2 2 2 0 0 2 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[2 0 1 1 1 2 0 1 1 2 0 1 1 1 1 1 2 0 0 2 2 1 2 2 2 1 2 1 1 1 2 1 1 0 2 0 1\n",
      " 2 0 1 2 1 1 0 1 1 0 2 0 1 1 0 0 2 2 1 1 1 1 1 1 1 0 1 0 2 1 0 1 2 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 2 0 0 0 1 0 1 0\n",
      " 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 2 2 2 2 2 2 0 0 2 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "(0.25620589479728606, 0.2970162926127069, 0.25672850848719775, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5733/5733 [00:02<00:00, 2280.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 7517.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  63388\n",
      "preparing embedding matrix...\n",
      "50000\n",
      "27295\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_26 (Embedding)    (None, 1000, 300)         15000000  \n",
      "                                                                 \n",
      " spatial_dropout1d_26 (Spati  (None, 1000, 300)        0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " bidirectional_26 (Bidirecti  (None, 64)               85248     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,086,356\n",
      "Trainable params: 15,086,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<BatchDataset shapes: ((None, 1000), (None, 4)), types: (tf.int32, tf.float32)>\n",
      "Epoch 1/40\n",
      "90/90 [==============================] - 12s 117ms/step - loss: 1.1358 - accuracy: 0.5627 - val_loss: 1.2846 - val_accuracy: 0.4000\n",
      "Epoch 2/40\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.8746 - accuracy: 0.6660"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d14281e2fba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/gpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         history = model.fit(train_dataset,\n\u001b[1;32m--> 118\u001b[1;33m               epochs=num_epochs, validation_data=test_dataset, callbacks=[es_callback], shuffle=False)\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_seq_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0my_pred_bool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "max_seq_len = 1000\n",
    "\n",
    "reports = []\n",
    "\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train_raw, y_test_raw = y[train_index].values, y[test_index].values\n",
    "#     print(X_train)\n",
    "    raw_docs_train = X_train.tolist()\n",
    "    raw_docs_test = X_test.tolist()\n",
    "    num_classes = 4\n",
    "    \n",
    "    for i in range(len(raw_docs_train)):\n",
    "        if 'www.' in raw_docs_train[i] or 'http:' in raw_docs_train[i] or 'https:' in raw_docs_train[i] or '.com' in raw_docs_train[i]:\n",
    "            raw_docs_train[i] = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"<url>\", raw_docs_train[i]).strip()\n",
    "\n",
    "    for i in range(len(raw_docs_test)):\n",
    "        if 'www.' in raw_docs_test[i] or 'http:' in raw_docs_test[i] or 'https:' in raw_docs_test[i] or '.com' in raw_docs_test[i]:\n",
    "            raw_docs_test[i] = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"<url>\", raw_docs_test[i]).strip()\n",
    "\n",
    "#     print(raw_docs_train[0])\n",
    "    processed_docs_train = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = tokenize(doc)\n",
    "        filtered = [word for word in tokens]\n",
    "        processed_docs_train.append(\" \".join(filtered))\n",
    "\n",
    "    processed_docs_test = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_test):\n",
    "        tokens = tokenize(doc)\n",
    "        filtered = [word for word in tokens]\n",
    "        processed_docs_test.append(\" \".join(filtered))\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
    "    word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "    word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "    #pad sequences\n",
    "    word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "    word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train_raw, num_classes = 4)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test_raw, num_classes = 4)\n",
    "\n",
    "#     print(y_train)\n",
    "\n",
    "    #training params\n",
    "    batch_size = 32\n",
    "    num_epochs = 40\n",
    "\n",
    "    #model parameters\n",
    "    num_filters = 16 \n",
    "    embed_dim = 300 \n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    print('preparing embedding matrix...')\n",
    "\n",
    "    words_not_found = []\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "    print(nb_words)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    #     print(word)\n",
    "    #     print(embedding_vector)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            #print(\"yes\")\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    print(len(words_not_found))\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "#     model.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=True))\n",
    "#     # model.add(Embedding(10000,300,input_length=1000,trainable=True))\n",
    "#     model.add(Bidirectional(LSTM(32)))\n",
    "#     model.add(Dense(32,activation='relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(4,activation='softmax'))\n",
    "#     model.summary()\n",
    "\n",
    "    model.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=True))\n",
    "    # model.add(Embedding(10000,300,input_length=1000,trainable=True))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(32, dropout = 0.2)))\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(4,activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((word_seq_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000, seed=my_seed, reshuffle_each_iteration=True).batch(64)\n",
    "    # tried this (shuffling, reshuffle), removed the issue with all predictions being one class, but still not helping scores\n",
    "\n",
    "    print(train_dataset)\n",
    "\n",
    "    # Prepare the validation dataset\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((word_seq_test, y_test))\n",
    "    test_dataset = test_dataset.batch(64)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        history = model.fit(train_dataset,\n",
    "              epochs=num_epochs, validation_data=test_dataset, callbacks=[es_callback], shuffle=False)\n",
    "        y_pred = model.predict(word_seq_test, batch_size=64)\n",
    "        y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "        print(y_pred_bool)\n",
    "        print(y_test_raw)\n",
    "        y_test_bool = np.argmax(y_test, axis=1)\n",
    "        print(y_test_bool)\n",
    "        \n",
    "        report = precision_recall_fscore_support(y_test_bool, y_pred_bool, average=\"macro\")\n",
    "        print(report)\n",
    "        # watch this, overwhelming class 3 (non rumor)\n",
    "        reports.append(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
